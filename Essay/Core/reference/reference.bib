@article{yaggi2006sleep,

  title={Sleep duration as a risk factor for the development of type 2 diabetes},

  author={Yaggi, H Klar and Araujo, Andre B and McKinlay, John B},

  journal={Diabetes care},

  volume={29},

  number={3},

  pages={657–661},

  year={2006},

  publisher={Am Diabetes Assoc}

}

@misc{haversine,
  title = {Calculate distance, bearing and more between Latitude/Longitude points},
  howpublished = {\url{http://www.movable-type.co.uk/scripts/latlong.html}}
}
@misc{earthdata,
  title = {Fire Information for Resource Management System (FIRMS)},
  howpublished = {\url{https://earthdata.nasa.gov/earth-observation-data/near-real-time/firms}}
}



 @misc{ enwiki:1065597644,
    author = "{Wikipedia contributors}",
    title = "Ball tree --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2022",
    url = "https://en.wikipedia.org/w/index.php?title=Ball_tree&oldid=1065597644",
    note = "[Online; accessed 11-February-2022]"
  }

  @article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@incollection{JORDAN1997471,
title = {Chapter 25 - Serial Order: A Parallel Distributed Processing Approach},
editor = {John W. Donahoe and Vivian {Packard Dorsel}},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {121},
pages = {471-495},
year = {1997},
booktitle = {Neural-Network Models of Cognition},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(97)80111-2},
url = {https://www.sciencedirect.com/science/article/pii/S0166411597801112},
author = {Michael I. Jordan},
abstract = {ABSTRACT
A theory of learned sequential behavior is presented, with a focus on coarticulatory phenomena in speech. The theory is implemented as a recurrent parallel distributed processing network that is trained via a generalized error-correcting algorithm. The basic idea underlying the theory is that both serial order and coarticulatory overlap can be represented in terms of relative levels of activation in a network if a clear distinction is made between the state of the network and the output of the network.}
}
@article{10.1162/neco.1997.9.8.1735,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}
@INPROCEEDINGS{861302,
  author={Gers, F.A. and Schmidhuber, J.},
  booktitle={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium}, 
  title={Recurrent nets that time and count}, 
  year={2000},
  volume={3},
  number={},
  pages={189-194 vol.3},
  abstract={The size of the time intervals between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While hidden Markov models tend to ignore this information, recurrent neural networks (RNN) can in principle learn to make use of it. We focus on long short-term memory (LSTM) because it usually outperforms other RNN. Surprisingly, LSTM augmented by "peephole connections" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps, without the help of any short training exemplars. Without external resets or teacher forcing or loss of performance on tasks reported earlier, our LSTM variant also learns to generate very stable sequences of highly nonlinear, precisely timed spikes. This makes LSTM a promising approach for real-world tasks that require to time and count.},
  keywords={},
  doi={10.1109/IJCNN.2000.861302},
  ISSN={1098-7576},
  month={July},}

  @article{kruskal,
 title={Minimum Spanning Tree Determination Program
Using Kruskal Algorithm on Visual Basic 6.0 },
 author={Nina Zakiah, Desniarti, Benny Sofyan Samosir},
 journal={International Journal of Science and Research},
 pages={1817--1821},
 year={2014}
}

